
# ğŸ¤– Go Ask Tasos

A locally running intelligent chatbot that either is modifiable or lets you query the content of your PDFs using Retrieval-Augmented Generation (RAG) and local LLMs via Ollama.

---

## âœ¨ Features

- ğŸ“„ Upload PDFs and automatically extract and index their content
- ğŸ§  Vector-based indexing stored as `.pkl` files per PDF
- ğŸ’¬ Chat interface  `llama3`, `gpt-4o-mini` (not connected to RAG) 
- ğŸ§ªâš¡ğŸ§Ÿ modify role and behavior
- ğŸ—‚ Choose previously uploaded PDFs without re-indexing
- ğŸ” Login-protected user access
- âœ… 100% offline, privacy-friendly, no external API calls

---

## ğŸš€ Installation

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/go_ask_tasos.git
cd go_ask_tasos


Attention:
ğŸ—‚ Required Folders
Before running the application, make sure the following folders exist in the project root:

uploads/: this folder stores uploaded PDF files.
vector_store/: this folder stores the vector index .pkl files generated from your PDFs.
If these folders are not present, you can create them manually using the terminal:

mkdir uploads vector_store
ğŸ’¡ Make sure these folders are at the same level as your main Python scripts (e.g., main.py, app.py).


2. Set up the Python environment

python -m venv .venv
source .venv/bin/activate  # macOS/Linux
.venv\Scripts\activate     # Windows

3. Install the dependencies

pip install -r requirements.txt

4. Install and start Ollama and pull models

Install Ollama if not already installed:
https://ollama.com  # for download

ollama serve

!!! Required models (must be pulled before starting the app)
ollama pull nomic-embed-text
ollama pull llama3

5. Run the application

python app.py

ğŸ•¹ï¸ Usage

Register or log in.
Modify your chatbot, and manage your hole communication.
or
Upload a PDF â€“ its content will be chunked, embedded, and saved as a vector index.
Alternatively: select an existing PDF from the dropdown list.
Enter a question â€“ the bot will respond using the context from the document.
ğŸ“‚ Project Structure

go_ask_tasos/
â”‚
â”œâ”€â”€ app.py                  		# App entry point
â”œâ”€â”€ project/
â”‚   â”œâ”€â”€ main.py                     # Flask routes
â”‚   â”œâ”€â”€ auth.py                     # Flask routes to authenticate
â”‚   â”œâ”€â”€ grogmodel.py                # grog model 
â”‚   â”œâ”€â”€ chatgptmodel.py.            # chatgpt model
â”‚   â”œâ”€â”€ models.py                   # db setub
â”‚   â”œâ”€â”€ ollama_rag.py               # RAG engine using Ollama
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ pimp-it.html            
â”‚   â”‚   â””â”€â”€ modifier.html  			
â”‚   â”‚   â””â”€â”€ prompt-area.html  
â”‚   â”‚   â””â”€â”€ login.html   
â”‚   â”‚   â””â”€â”€ index.html   
â”‚   â”‚   â””â”€â”€ base.html   
â”‚   â”‚   â””â”€â”€ signup.html      
â”‚   â”œâ”€â”€ static/                     # CSS 
â”‚   â”œâ”€â”€ uploads/                    # Uploaded PDFs
â”‚   â”œâ”€â”€ .env/                       # place your api keys in here
â”‚   â””â”€â”€ vector_store/               # Saved vector DBs (.pkl)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
âš™ï¸ Configuration

Models in ollama_rag.py are set on default:

EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'
LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'


âœ… To-Do

 Support for .txt, .docx and other formats
 Multilingual chat answers
 Export chat history
 REST API (OpenAPI / Swagger)
 
âš ï¸ Limitations

Only supports local models available via Ollama
Does not re-index if a PDF is modified after upload
Embeddings and answers depend on Ollama's local runtime


